Azure AZ-900
=============
https://www.udemy.com/course/azure-certification-az-900-azure-fundamentals/learn/lecture/26525596#overview

One of the primary reason we go for Azure(or any cloud) is to be able to deploy our application in multiple regions around the world. 
	-> The concept that helps to achieve above is known as Regions and Availability Zones in Azure.
	
Where do you deploy applications to in the cloud? 
--------------------------------------------------
	-> That is Virtual Machines, azure service that helps to create virtual machines is "Azure Virtual Machines".

In section 3, we will learn about how to create virtual machines? installing softwares? create multiple virtual machines? distribute the load between them?

Creating virtual machines? load balancers to balance the load between them is manual process, that we would make automatic with the help "Managed Compute Services" in Section 4.
	-> we would learn the fundamentals of IaaS, PaaS, SaaS and also some more important Managed Compute Services provided by Azure.
		-> Azure App Services
		-> Azure Container Instances
		-> Serverless with Azure Functions
		-> Shared responsibility model in Azure
		
In Section 5 - We would learn about different storage options provided by Azure - "Azure Storage"
Section 6 - Azure Databases (RDBMS, NoSQL) and related services provided by Azure.

Section 7 - Azure Networks(Azure virtual network, Firewall,subnets, Network security groups) and other services offered by Azure and some more best practices.

Section 8 - Organizing Azure Resource groups.

Before cloud challenges;
------------------------
	-> Peak load provisioning (Buy infrastructure for peak load(that we think max load))
	-> But always it would not be running in peak load, then huge resources setup would be sitting idle that time, most of the times during holidays, festivals it would be used peak load.
	-> Needs ahead of planning (as we would not know when our app would reach high and peak usage among customers) , hard to guess the future?
	-> When infra setup done, we need to employ infra maintenance team (would be costly for a startup to afford)		
		

Think about Cloud?
-------------------
	-> You don't need to pre-setup for Peak load, you will just need to increase the resource as the load increases, it goes vice versa when load decrease. CAPITAL EXPENSE changes to VARIABLE EXPENSE. Helps on-demand resource provisioning.
	-> which obviously helps the problem that we don't need to plan way ahead. Stop guessing the future.
	->  Go global in minutes. (just on a click app is deployed to multiple regions around the world!!!)
	-> Stop running and maintaining Data Centers.
	
Microsoft Azure
----------------
		It provides 200+ services. 
		Proved it as reliable, secure, cost-effective.
	
Account created, not sure but I was not able to create free account, then finally created account with Pay-as-you-go with gowthamn91@outlook.com.

										"Beware of it"
										
WHY REGIONS and ZONES?
=======================
Imagine I have an application hosted in a Data Center in LONDON.

What would be the challenges?
	1. Slow access for the user from other parts of the world (like Sydney, Mumbai, Chennai, NY etc.,) This is called a HIGH LATENCY.
	2. Right now our application is deployed in only data center, what if the data center crashes? Application goes down and unavailable. (LOW AVAILABILITY)

How do we solve these problems?
--------------------------------
	-> For 2nd problem, if we add one more data center in LONDON REGION, if one data center crashes, the other one can still serve the application. (SOLVES #2).
		-> But what if the entire London region goes down, then it is still a problem to be solved?
		-> To resolve this, lets add more region to our application, we deploy application to another region, this time lets assume "MUMBAI REGION", this makes our application to be HIGH AVAILABLE. if LONDON goes down, MUMBAI REGION would still serve our application users.
		-> Actually this approach would solve the first problem as well in a way if we deploy our applications in multiple regions around the world, so that all the users would be served by the Data Center closer to them which means HIGH LATENCY also resolved.

ADVANTAGES OF REGIONS AND AVAILABILITY ZONES
---------------------------------------------
	From the previous examples and details, it should be prettty clear that we have the below advantages of using Regions and Zones.
		1. High Availability
		2. Low Latency
		3. Global foot print for any start up to reach global within minutes and serve users across world.
		4. Helps to adhere to local government regulations, like we don't want to take any users data out of the particular country and store in that respective data center itself.

With Availability zones;
-------------------------
		1. High Availability and Fault tolerance within the region.
		 	
	Mostly Availability zones will have independent power, networking & connectivity to ensure failure in one data center does not affect the other data center (zone), hence any failure would make the other data center to serve the application with out much issue.

		Point to note is; not all the regions have availability zones, some has upto 3 AZ, some have nothing as well.
	
AZURE VIRTUAL MACHINES:
=======================
	Where do we deploy our applications? Typically in a Server, but in case of Cloud, we deploy it in Virtual Server.
	How do we provision Virtual Server? It is with the help service called - Azure Virtual Machine.
	In different Cloud Providers it is termed differently (like Compute Engine in GCP, EC2 in AWS). EC2 - Elastic Compute Cloud.

	Features offered by Azure Virtual Machines Service:
	---------------------------------------------------
		-> Create and Manage life cycle of the VM instances.
		-> Load Balancing and Auto Scaling for multiple VM instances based on the users.
		-> Manage Network Connectivity and Configuration for VMs which helps us to connect with VM and access applications hosted in it.
		-> Attach Storage to your VM instances.
		
Creating a VM from Portal:
---------------------------
			Search for Virtual Machine Resource and create.
			-> While creating VM we will need to create Resource Group under which this VM should go in.
			-> We also select image of OS.
			-> And the Region where this VM should be deployed to. There are many Regions will be listed, we can choose one based on the needs.
			-> Then the size of the VM (RAM, CPU, Data Disc that can be attached to)
			
			-> Also select an Http(80) inbound port so that when we install web application to this VM, through this port we will access our application.
			
			-> Review + Create -> runs validation -> once successful -> click Create button.
			
OK. I was able to create a VM and connect to it via SSH (Secure Shell) with help of "bash" there is an icon which opens Cloud Shell in Azure Portal. 

I had to first upload my private key file downloaded from the Cloud Shell.
Then I had to follow some commands, it was also listed in the Portal. Below are sample.		
			
Here are some of the commands we would execute in the next few steps:

You can refer back to this if you have problems. Good Luck.

chmod 400 my-first-vm_key.pem
ssh -i YOUR_KEY_PATH azureuser@PUBLIC_IP_ADDRESS (ssh -i my-first-vm_key.pem azureuser@52.190.58.93)

sudo su - before running the install command, make it a super user. 
apt-get -y update - updates packages
apt-get -y install nginx - installs nginx and launch it.
 
echo "Hello World"
echo "Hello World" > /var/www/html/index.html
echo "Hello World from in28minutes" > /var/www/html/index.html
 
hostname
echo "$(hostname)"
echo "Hello World from $(hostname)"
echo "Hello World from $(hostname)" > /var/www/html/index.html

KEY CONCEPTS OF VIRTUAL MACHINES:
---------------------------------
	1. Image - OS of the VM
	2. VM Family - if the VM is General Purpose/High Performance Compute/Memory Optimized
	3. VM Size - VM of same family can have different sizes like (no. of vCPU, RAM size, Disk storage and more.
	4. Disks - Attach Virtual Disk similar to HDD attach to laptop - this is done while creating a VM, we have a tab named "Disks" there we can create or attach an existing disk to the VM - A Data disk can also be created from Disks resource.
		-> There are different storage types available (Premium SSD, Standard SSD, Ultra Disk, Standard HDD).



Installed a web server to our VM - my-first-vm - nginx

nginx - is a popular web server similar to IIS (mainly for windows based apps), but nginx is light weight and cross platform supported.		

After running the command - apt-get -y install nginx - it installs and launches the webserver.
Post that if I hit the public IP: http://52.190.58.93/ - I am able to see the message whatever I echo from console.

	echo "Hello World" > /var/www/html/index.html - Displays "Hello World" in browser.
	
	One wonderful thing is I am able to open this page from my mobile as well. Thats Fantastic!!!!!!	
									POWER OF THE CLOUD!!!!!!!!!!!!!!!!!!!!
	
Now instead of doing these installation of nginx manually, we can have it automatically done on creation of the VM, we need to provide the above script that I add once below;
	
		"	
			#!/bin/sh - to mark it as bash script
			sudo su 
			apt-get -y update
			apt-get -y install nginx
			echo "Welcome to Gowthaman's VM $(hostname)" > /var/www/html/index.html
		"	

Then on creation it automatically installs nginx and launches it. May need to wait for few mins before launching the public IP address to check the message from browser.	

When we want to increase availability for the application?
-----------------------------------------------------------
	When we have only one VM instance running, the availability may depend on the hardware or disk that we use, for instance;
				-> Premium SSD or ultra disk - 99.9%
				-> Standard SSD Managed Disk - 99.5%
				-> Standard HDD Managed Disk - 95%
	
	If we need more availability, we can have two or more instances of VM running in Same Availability Sets	(grouping of resources based on Fault Domain and Update Domain)
		Fault Domain: The area of the data center which uses same power source and network Switch.
						It talks about physical boundary for the resource/instances so that the resources are spread across multiple fault domains to avoid any particular fault domain's failure does not affect all the instances and help us with high availability.
						
		Update Domain:
					  It mainly talks about logical grouping of resources based on the rolling out of the updates.
					When application is undergoing update process, not all the instances in the	
					update domain goes simultaneously, it goes through one by one which helps us in high availability when one update domain goes update process, the other instances in another update domain keeps serving the users.
					
	When needed even more availability, we need to have two or more instances of VM running in different Availability Zones in the same region: it can provide us about 99.99% of availability (four 9s) not so easy to achieve but most of the online apps aim for this.

Then to delete the VMs Created before going to next step:
---------------------------------------------------------
	Go to the respective VM - my-first-vm -> Delete -> Select all associated resource types -> (OS, Network interface, IP addresses)  
	
VIRTUAL MACHINE SCALE SET:
===========================
  When we want to create and manage multiple VMs(Identical or VMs of different types) , VMSS is the Azure tool helps us doing that.
	-> We know when we have multiple VM instances running, the availability of our application increases, and when we have distributed VMs across fault domains, update domains, the availability goes even higher, and when it is distributed across multiple AZs then it is even more higher availability.	
	-> Exactly VMSS helps us in this, since it automatically distributes the VM instances across fault domains, update domains.
	-> That helps high availability during any hardware failures or maintenance events.	
	-> Scaling: We can configure manual or auto-scaling to ensure the VM instances scale in or scale out based on the load. It obviously helps in managing cost effectively.
	-> Also helps us setting up the Load balancing for the VM instances and help managing the load efficiently by not overloading any particular Virtual Machine.
	
Have created a Virtual Machine Scale Set (vm-scale-set-1)
	Able to see 2 instances created.
	it also has created a Load Balancer.
	
	When we do curl IpAddressOfLB - we can notice it does round robin of passing request to two instances and gets response.
			
			"curl 57.151.52.221" (LB IP)
			
	We can also do manual scaling from VMSS by adding number of instances to be added to the scale set.
	Once we add and save, it automatically creates the instances and we can see it in Instances screen as well.
	May take a few mins, post that curl command we can see the request goes to newly created VMs as well.

Static IP Address:
-------------------
	One of the features of Virtual Machine, what is the need for static IP Address?
	We know when we create a VM, we can have public IP address created which are basically incur charges per hour, but one thing to be noticed is when we stop the VM and start it again, we can see this time new IP address created other than what was created before, this is because when we created a VM, it must have been selected as dynamic

    But for me whatever VMs I created it had only static IP Assignment. so it does not get changed for any time stop or restart.
	
	Since these IP address are charged per hour, best practice is to release it when it is not used.
	
Similar to the above, there are below features as well;

Azure Monitoring - For monitoring the VMs with different metrics.

Dedicated Host - Physical VMs in the host would be particular for a single customer. Otherwise A single host can have multiple VMs and those VMs can be for different customer applications, Azure would still ensure the VMs are having enough security measures and isolated so that there won't be any insecure access, still the VMs are part of the same hardware/resource.

If suppose we need to have a dedicated host for our servers(VMs) then we can go for dedicated host.	

Azure Spot Virtual Machines 
----------------------------

	-> These are mainly for workloads which are fault tolerant and are non mission critical.
Azure Spot VMs gets allocated based on the market capacity and users specification, and also to notice whenever there demand arises Azure can withdraw the VM and give only 30s of notice time.

Reservations:
--------------
	We can reserve resources ahead of time for next 1 to 3 years as well with discounted price.
	
Designing Good Solutions with VMs:
----------------------------------
	When designing solutions in Azure, several factors need to be thought through;
		Availability 
		-------------
				-> whenever users try to access our application, it is available for expected use.
						-> Achieved via Availability Sets, Scale Sets, Availability zones
						
		Scalability
        ------------
				-> when the demand grows performance of our app should not go down, it should be able to be served with out any interruption. 
					 -> It is achieved by two ways; Vertical, Horizontal.
					 
			Vertical:
			---------	
				=> Increases the size of the VM to serve more, but it has a limit that only to certain limit it can be increased. (RAM, Disk, CPUs).
					-> It can be more expensive, buying a heavy memory or CPU can cost high.
			
			Horizontal: 
			-----------
				=> This is the thing we had seen earlier as well with Scale sets, by adding more instances of Identical or varied configuration. And we can use Load Balancer to route the requests to VMs based on the load.
					-> This is the most preferred one. But to notice it involves additional infrastructure as it needs Scaling Sets, Load Balancers as well.
					-> With this we get the availability as well, as any VM failing, app would still be served from other VMs, thats not the case with Vertical Scaling.					
					
		Resilience:
		-----------
				=> Ability of System to provide acceptable behavior when one or more parts of the system fails.
				   Achieved via Scale sets and Load balancers.
				   
		Geo-distribution:
		-----------------
				=> Distribute application across regions and zones (Scale Sets and Load balancers)
				
		Disaster Recovery:
		------------------
				=> How to keep systems running in case of disaster (Site Recovery)
					-> We can use site recovery to configure secondary region and whenever there is a problem with primary region, would have performed disaster recovery and VM would be copied over to secondary region.
				
		Managing Costs:
		---------------
				=> With help of Auto Scaling (Elasticity), Reservations, Spot Instances 
					
		Security:
		---------
				=> Secure your VMs - (Dedicated Hosts)
	
MANAGED COMPUTE SERVICES:
==========================
		Something like Azure App Service helps us not to worry much about anything like Virtual Machines, deployment, load balancing, we just tell App Service this is the web application I want to run, it just takes care of everything for us.
		In this section, we would learn about IaaS, PaaS, Containers, Kubernetes, Serverless, Azure Functions.
		
	In the previous section, we have seen creating a VM, installing software(nginx), configure load balancing, auto scaling but setting those were not easy, involved more manual steps. How can we make it simple? thats where we go for Managed Services.
		
		There are different terms that we need to understand before;
			IaaS - Infrastructure as a Service	
			-----------------------------------	
					-> Good example is the use we have already seen, Cloud just provides us with physical hardware, virtualisation, networking, where we have created a VM, installed OS, install softwares, deploying application, configure load balancing and availability and Application code and run time(.net).
					-> Also any upgrade or patch related things to OS we have to take care of.
					
			PaaS - Platform as a Service
			-----------------------------
					-> Here in this case, we will use a platform provided by Azure, where we will not need to worry about Installing OS, Softwares, respective patches/upgrades, AutoScaling, Availability, Load Balancing, all these are taken care by Azure.
					-> Our responsibility is only related to Application configuration and code.
					-> Rest of all the other things taken care for us.
				Examples:
				----------
					Azure App Service.
					Azure SQL database - Relational and NoSQL
					
We will start with Azure App Service in the next section:
---------------------------------------------------------
	It is a fully managed platform for building, deploying and scaling our web apps.
	It natively supports .Net, .Net Core, RestAPIs, node.js etc.,
	We may need to choose a App Service Plan - defines set of compute resources for a web app.


Lets create our first web app using Azure App Service.

Go to App Services -> Create -> Web App -> Since I deleted compute-rg, create a new rg (compute-rg-new) -> Add a app name (my-first-web-app-gowthamn91) -> select if we are going to publish code or docker container -> I selected docker option -> region selected as sweden central -> App service plan with free usage (ASP-computergnew-9f13 (F1: 1)) recommended already -> Docker tab, auto selects NGINX to be installed automatically.
					
Default domain: https://my-first-web-app-gowthamn91.azurewebsites.net/ -> up and running.

INTRODUCTION TO DOCKER Containers
----------------------------------
 -> One of the most popular architectural styles today is Micro Services.
 -> Why micro services? it helps to build small set of focussed services working together to form the system instead of building one large monolith application.
 -> One big advantage is it provides "Flexibility to innovate", like we can build different services in different programming language.
	-> Like Movie Service in Go, CustomerService in Java, BookingService in Javascript etc., 
	-> Ideally we will not want to use lot of languages to be used in a single enterprise as it adds more complex while deploying the small applications/service as Go might have different way of deploying, Java has different way.
	-> But to note this microservices gives the flexibility to choose different language if a real need arise for building any particular service.
	
How do we avoid the challenges with deploying these small applications?
------------------------------------------------------------------------
		Though different languages used, how do we make it one way of deploying applications.
		Here comes the CONTAINERS play a crucial role.
		
		For each microservice, it creates a Docker image (which consists of all the things needed for the service to run) like Application runtime, if Java then JDK, if python - python inside the container etc.,
			-> Application code and its dependencies.
			
	VM vs Container
	---------------
		VM - virtualizes hardware and each VM can run on its own OS, CPU, Memory, Network interfaces.
		It is useful for the applications which require OS-Level Isolation, compatibility with specific OS versions.
		
		Containers: Shares host OS Kernel, shares the host resources (CPU, memory and networking).
					It has lesser over head than VMs because of it needs all seperate resources.
					Even containers provide isolation at the light weight level between containers.
					It is well suited for smaller , microservice based applications.
					
		Docker is Cloud Neutral. (it can be deployed in any cloud environment).

Creating First Container:
-------------------------.
Go to Container instances -> Container name, quick start image (hello world) -> resource group -> Create -> once done we can copy the IP address and load the IP in browser -> to view the Azure Container instance up and running

		
Managing Multiple Containerized applications
--------------------------------------------
	If we have built a container images for our Microservices, for example containers build for Microservice A, B, C, D and E. Now we want to manage the deployment of these containers, also we want to auto-scale based on the load.
	But to have this, just having containers would not be sufficient, we would want some kind of orchestration around these containers.

		Thats where we come across different orchestrator solutions, most popular one is "Kubernetes" - open source container orchestrator solutions, which helps us in managing the deployment, auto-scaling of the container instances based on the load, resiliency that identify if any container instance fails and replace it with working instance, and whenever we talk about multiple instances Load Balancer is a must in place to distribute the load between the instances.
		
		Other features are; 
		-------------------
			Service Discovery 
			-----------------	
				-> Each container can ask orchestrator for the location of the other container, so that no need of hardcoding the URLs of the other microservices.
			Self healing 
			------------
				-> Do regular health check on instances and replace failing instances.
			
			Zero Downtime Deployments:
			---------------------------	
				-> Orchestrator provides number of different strategies to release new versions of software without downtime.
				
	All the Cloud Providers provide Kubernetes Managed Services.
				
								EKS - Elastice Kubernetes Service (AWS)
								AKS - Azure Kubernetes Service (Azure)
								GKE - Google Kubernetes Engine (GCP)

	Now lets see about AKS and Service Fabric provided by Azure.
	AKS - Managed Kubernetes Service
	---------------------------------	
			-> Whenever we use orchestrator, we create the cluster and configure the number of nodes or servers and deploy microservices.
			-> since this is offered by across cloud providers, when we need orchestrator solution to be cross compatible then AKS is the option to go for.
			
	Azure Service Fabric:
	---------------------
		This is a Microsoft specific orchestrator solution.
		
SERVERLESS:
-----------
	When we develop an application, we think about language, framework to use, along with it we also think about where to deploy my application into, which OS?
			But if suppose we don't want to worry about all these servers, deployment, scaling, availability aspects, there comes Serverless feature.
			
	Serverless - does not mean "No Servers". It still deployed into a server, but we don't have any kind of visibility about the infrastructure of where the application is deployed to. 
		-> Example: Functions as a service. In Azure - Azure Functions.
	Also it offers Pay for use -> if no requests, then no need to pay, pay only for invocations/requests, not for servers.
		Basically, we pay for;
				=> Number of requests
				=> Duration of requests
				=> Memory Consumed
				
Create a Function App:
-----------------------
		Go to FunctionApp -> Create a function app -> add details like name, runtime stack -> then most of the other tabs with default values -> review and create.
		We can add the function as HttpTriggers, Queue Trigger, etc., - code and logic and also there is a public url which can be called from browser to access the function for HttpTriggers.
		
		This Azure feature also offers always free tier, that is 1 million request per month is free, we pay only for what we use.

Software as a Service (SaaS)
-----------------------------
		It is a centrally hosted software, mostly on the cloud (like gmail, office365, docs etc.,)
		As in IaaS - we have more responsibility even configuring the hardware, PaaS - we don't care about hardware, just focus on the application logic and development, in SaaS - it is the software provided for use pay-as-you-go, here users responsibility is just about configuring and using and the Data. Everything else taken care by the cloud provider.

Shared Responsibility Model:
----------------------------
On Prem - Including physical farm/server setup
IaaS - Other than physical farm/setup, responsibility on others OS, network, application, identity management, data.
PaaS - No more responsibility on OS configuration, but partial on N/w, Applications, Identity and directory infrastructure 	and data.
SaaS - No OS, n/w, application, only accounts, data, device.

AZURE STORAGE
==============
	In previous section, we have learnt about Compute Services, in this we will go ahead learn about Storage.
	There are three basic storage types;
		Block
		-----	
			-> like out hard disk storage
			-> Typically, one disk can be connected to one VM
			-> But a VM can have multiple disks connected.
		File
		-----
			-> If suppose we want to share a file with our colleagues in an enterprise, there we go for File Storage type.
		
		Object
		------
			-> In both the above storage types, we have to mount the storage on to the Virtual Machine/ establish the physical connection with the VM.
			-> What if we don't want to mount the storage to our VM, and we want to upload / download the files to a storage using a REST API in a more decoupled way, there comes the Object Storage. 


	There are even more wide variety of Storage Types. 
	
	In Azure, where we would store all the different types of storage is Azure Storage.
	Azure Storage is Managed Cloud Storage Solution, It is highly available, durable, massively scalable.
	Whenever we talk about data, Durability is very very important, you don't want to lose data.	
	Scalability is about how much data that can be stored in Azure Storage - (upto few petabytes)
							
							1 Tera Byte - 1024 GB							
							1 Peta Byte - 1024 TB		
					Which is huge huge amount of data!!!!!!!!	
	
	Core Storage Services
	----------------------
		Azure Disks: Block Storage
		Azure Files: File Storage
		Azure Blob: Object Storage (Text file, binary data)
		Azure Table: NoSQL (very basic features)
		Azure Queue: For messaging system (which decouples producer and consumer)
		
	But to make use of the above services, we need to have a storage account created.	\
	
How do we achieve the Durability?
----------------------------------
Data Redundancy - By replicating the data in multiple locations.

LRS - Locally Redundant Storage - three synchronus copies of data stored in the same data center. (default in any storage)
	-> Least expensive and Least availability since same data center.			

ZRS - Zone Redundant Storage - 
	-> Data is copied on to three different AZs of the primary region.

GRS - Geo Redundant Storage	
	-> LRS + Asynchronus copy of data in secondary region (three more copies using LRS)
	-> Meaning LRS in both primary and secondary region

GZRS - Geo Zone Redundant Storage
	 -> ZRS + Asynchronus copy of data in Secondary region (three more copies using LRS)	
	 -> Most expensive and high availability, durability
	 -> ZRS in primary and LRS in secondary region.
	 
Region Pairs
-------------
	We have seen above that to achieve high durability, need to replicate data across regions. 
	But how it is done? using Region pairs.
	When we create the storage in Azure, we select the region and all the regions in Azure have a region pair tagged which is not more than 300Kms away so that the transferring of data is also faster.

					"Like Central India and South India, East US and West US"
					
	Also Azure ensures that both the regions will never go down together, very very rare to see both the regions go down. They ensure updates/patches are applied on different times for the region pairs to ensure high availability.
		
		-> Always data access happens from Primary region, option to failover to secondary region when primary region is not available.
		-> Since these region pairs are mostly near to each other, not more than 300kms away, data replication and synchronization happens very fast.
		
MANAGED AND UNMANAGED BLOCK STORAGE:
====================================
	We know what is a block storage, it is like hard disk connected on our computer. 
	Here disks attached to a VM. 
		-> Typically one hard disk can be attached to a single computer, but single machine can have multiple disks attached to it.

Where we can create/add disks to VM?
-----------------------------------------
	Create VM -> Go to Disks tab -> Here we can select disk size and disk type as below;
			Standard HDD - best for less frequent accessed data, best for backup, non critical.
			Standard SSD - best for web servers, lightly used web applications.(dev/test environments)
			Premium SSD - Best for production and performance sensitive loads.
			Ultra SSD - Best for IO intense workloads, transaction heavy workloads. (SAP HANA, SQL, ORACLE)
			
		Premium SSD and Ultra SSD provides more availability upto 99.9%
		Standard SSD - 99%
		Standard HDD - 95%
			
	Whatever disks we have seen so far are Managed Disks, when we create the VM, we go with default option of storage disk, that is managed, that is the recommended to be used.
		There is also another type of storage disk - Unmanaged Disks.
		
	Difference between Managed and Unmanaged:
	------------------------------------------
		-> In managed, we just create disk, but we do not worry about where the disk is stored and its availability, durability all these things will be automatically taken care by Azure for us.
		-> In Unmanaged, we have to configure all these things manually and those caonfigurations can be very tricky, hence it has to be avoided if you can.
		
How to create Unmanaged disk?
------------------------------
		Go to Disks - before that make sure the VM selected is of Gen 1 configuration -> Selected HDD as we are just doing trail with pay-as-you-go to avoid more pricing -> Advanced tab -> uncheck managed -> it will create another storage account -> create VM (takes about 10mins)
		
		But whatever disk created is not visible in the Disks, but can be seen inside the storage thats created -> inside that there is containers where we can see the new unmanaged disk created.
		This container is just a term, nothing to do with Docker Container.
		Blob type is of "Page Blob".
		
Azure File Storage:
--------------------
	When we want a storage which can be accessed by multiple VMs/Devices to fetch files/store files, then File Storage is our option.
		-> The devices which accesses the file storage can be from Cloud or from OnPrem, can have any OS (Windows, Linux, macOS).
		-> When we talk about file storage, they make use of two important protocols SMB(Server Message Block) and NFS(Network File System).
		-> "Azure Files" supports both these protocols.
		
Create a Storage Account -> Then go to storage account resource created -> Data Storage -> 
			You can find different options there;
				Containers -> Where unmanaged disks stored/created
				File Shares -> File share can be created.
				Queues
				Tables
				
		After file share created -> We can connect to it from different OS, instructions are given there.
May need to try it.		
	
Azure File Sync:
=================
	"Windows File Share" is the tool to create file shares for on-prem users, system, it gives number of flexible options to connect to and access the data in the share path.

	We have seen earlier that "Azure Files"	is the service to create file share in the Cloud, one of the advantage is it exist in the Cloud, it can be connected from any device. 
	But it does not provide the flexibile different ways of connecting as in the Windows File Share.
	
	If we want to host the share path in the cloud and  also need the flexible ways of connecting provided by Windows File Share, then "Azure File Sync" service is the way.
	
AZURE BLOB STORAGE:
===================
	It is object storage in Azure. 
	What does it store? helps saving massive amount of unstructured data like (text file, image files, media files).
	Structure: Storage Account -> Countainer(s) -> Blob(s)
	It is highly scalable, provides data resiliency, data security with features such as RBAC and integration with Azure Active Directory.
	
	Different types of BLOB:
	------------------------
		Block Blob:  Stores Full text,image, media files, DB backups, Archives.
		Append Blob: Suited for Append operations, that is for Log file.
		Page Blob: 	 Virtual Disk that we attach to VM. Foundation for IaaS disks.
		
	Azure Data Lake Gen2 Storage:
	------------------------------
		-> It is also Azure Blob Storage, but it combines with additional features of storing data in both structured and unstructured format by storing the data in hierarchical system with directories and folders. And this helps in managing data and performing Data Analytics using technologies Azure Databricks, Azure HDInsight, Azure Synapse Analytics (formerly Azure SQL Data Warehouse). 
		
		
	Access Patterns and Access Tiers:
	----------------------------------
		Though we store the data, we have several patterns when/how frequent we access a particular data, if we access a particular file/data very frequent then we want that file to be access with low latency.
		
		That is Hot Tier. Low Latency, high storage cost.
		
		When a file is accessed less frequently probably every 30 days, that can be go into Cool Tier. - High Latency than Hot Tier, low storage cost than Hot Tier.
		
		Suppose a file is just stored may not be accessed for months, may be 6 months once. That goes into Archive Tier. And you know it, High Access Cost, for accessing / downloading the file it may take few hours of time, Low Storage cost.
		
	Create Storage Account -> Containers -> Add Container -> Start uploading files -> Here you can change the tier (hot, cool archive). Can have multiple versions.

	Container Properties to view the URL that can be used by different applications to Save/Fetch the data.
		"https://myfirststorageaccountgow.blob.core.windows.net/myfirstcontainer"
		
	May need to have some authentication details.

		Shared Access Signature / Shared Access Tokens		
		----------------------------------------------
			-> It is a URI that grants restricted access to the container for predefined time range without sharing the account key.
			

AZURE STORAGE EXPLORER/BROWSER:
================================
	This is similar to Windows File Explorer, where we can manage files, similar here we can manage files, accesses/permissions, Upload, Download files.
	
	It also provides with extensions to move data from another cloud to Azure like 
AWS S3 to Azure Storage.

	This helps to perform these operations with GUI, but if suppose we want to have it done by Command Line Utility and automate these operations, then we have something called "AzCopy".
		Even Azure Storage Explorer does use AzCopy in the backend.

AZURE DATABASES:
=================
	We have learned about Compute Services and Storage Services in the previous sections.
	In this we will start learning about Databases, before that we will go through some of the fundamentals of the Databases like availability, durability, consistency, then we go through different kinds of Databases (SQL, NoSQL, etc.,).

	Basically database provides organized and persistent storage for our data.
	Before choose a DB which type to be used for different purposes, we need to understand some of the basics like;
		=> Availability
		=> Durability
		=> Consistency
		=> RTO
		=> RPO
		=> Transactions etc.,
		
 For example:
 ------------
	We have a DB server in London region which our application connects to.
	Challenges:
	-----------
		1. DB server goes down, if the data center goes down.
		2. Data loss, if the database crashes.

	How do we resolve these challenges?
	------------------------------------
		We can have a Database Snapshot stored in another data center every hour, so that when there is down in primary data center, we can restore the data using snapshot.
		
			1. Still remains if there is a data center failure.
			2. But with data loss, helps us partially based on when the DB crash occurs, if suppose Snapshot taken at 4 PM, DB crashes at 4.50PM, that 50mins of data is lost.
				-> With this there is another problem? whenever we take snapshot, the DB performance goes very slow.
			
		Can we resolve these 50mins data loss?
		---------------------------------------
			Yes, that is done with the help of setting up Transactional Logs, which keeps of each transactions, along with Point in time backup of Snapshot, we can restore almost data without any loss during DB crash.
	
		Still the performance slowness is a problem?
		--------------------------------------------
			So now, we have replication DB server setup at the Data center 2, which replicates synchronusly, and snapshot is taken from secondary DB server.
				-> Now you can see two of the problems solved, 1st one when data center failure, DB goes down, here we have secondary DB server to serve us!!!!! and 2nd one with respect to performance slowness during snapshot, since primary server does not involved in Snapshot process, there is no issue to our application.

AVAILABILITY and DURABILITY:
----------------------------
	Availability - is about DB is available for my access now and whenever I need.
				 - Typically, it is measured in % as below;
				 99.95% - Application has downtime of 22 mins in a month.
				 99.99% (4 9s) - Application has downtime of 4.5 mins in a month. This included release downtime as well.(All the online apps aims for 4 9s).
				 99.999% (5 9s) - Application has down time of 26s in a month (highly tough to achieve)
					
 	Durability - is about how long my data is stored safely
			   - If i can access my data 10 years later/100 years/ more than that as well, because we hate losing data, once the data is lost, that is it its gone permanently.
			   - we are even ok with application downtime for few mins in a month, but data loss highly not acceptable.
				
				99.999999999 (11 9s) is considered to be good durability.
					=> means, if we store 1 million files for 10 million years, we expect to lose one file.
		
INCREASING AVAILABILITY and DURABILITY:
=======================================
	How do we increase Availability?
	---------------------------------
		-> This we have already known, we need to have multiple standby systems setup, so that when a server goes down, it switches over to another standby to serve the application.
		-> To distribute the servers across multiple AZs or multiple Regions.
		
	How	do we increase Durability?
	-------------------------------
		-> We need to have multiple copies of data (standbys, snapshot, transaction log, replicas) in servers across multiple AZs / Regions.
		-> But replicating data comes with its own challenges, like how do we maintain the consistency of data across different data copies stored.
		
			That we will discuss in the later section.
			
RTO and RPO:
============
	RTO - Recovery Time Objective - Maximum acceptable downtime.
	RPO - Recovery Point Objective - Maximum acceptable period of data loss.

	We have seen about Availability and Durability are technical measures of data availability for access and persistency of data stored.

But if suppose, there is a server went down, how do we measure how quickly we can recover from the failure?
	There comes RTO and RPO.

Example:
---------
	You are running an application on VM instance storing its data on a persistent storage.
	You are taking snapshots every 48 hours, if the VM instance crashes, you can manually bring it up in 45 mins from the snapshot. What is its RTO and RPO?
	
Ans: RTO - 45 mins, RPO - 48 hrs (if server down at 47h 59m, prev back up taken before 48hrs, data loss of about 47h 59m)

Scenario:
----------
Very small data loss - RPO(1 min) 
Very small downtime - RTO (5mins) 
	-> Then we have to go for Hot standby, where the redundant system is fully operatational, synchronusly replicated and ready to take up the load immediately when the primary server goes down.

Very small data loss - RPO(1 min) 
little downtime is acceptable - RTO (15 mins)
	-> We can go for Warm standby, where the redundant system is partially operatational, automatica synchronize data, it won't immediately ready to take up the load when primary server goes down, takes slight amount of time to start picking the load once primary goes down.

Very small data loss - RPO(1 min) 
few hours downtime is acceptable - RTO (few hours)	

	-> We can have the snapshot, transaction log setup, so that once there is a server down, we can setup the database using snapshots, transaction logs within few hours.
	
Data can be lost without a problem:
-----------------------------------
	for eg: cached data - Failover to completely new server.	
	
CONSISTENCY:
============
	How consistent the data across different replicas?
	If we need all the data to be immediately availabile on all standbys, then we go for STRONG CONSISTENCY.
		-> but the problem with it is, when we have multiple standbys, the transaction gets committed only when all the replicas got the data update, and it slows down the system.
			-> Suits for critical systems like Bank transactions...
			
	Then We have something called as EVENTUAL CONSISTENCY, there would be slight delay in replicas get updated with the data, but eventually all the replicas would get updated.
		-> Eg: Social media posts, Facebook post, instagram posts, etc.,
		
	Read-after-Write Consistency
    -----------------------------	
		Inserts are immediately availabile to replicas, but updates would be Eventual Consistency.
	
	Different DBs offer different level of consistencies.
	
How to choose the Database?
----------------------------
	There are several categories of Databases, 
		-> Relational Databases 
			-> Even in this there are several categories;
				-> OLTP (Online Transactional Processing)
				-> OLAP (Online Analytics Processing)
		-> In memory 
		-> Document 
		-> Key value
	
	Different factors to choose databases;
	--------------------------------------
		-> Fixed Schema (NoSQL offers flexible schema or schemaless)
				-> this will have predefined tables, columns, data types, constraints (SQL)
		-> Transaction Properties 
				-> ACID 
					-> Atomicity - Transactions are completed in full or not at all, like if any part of transaction fails, entire transaction is rolled back.
					-> Consistency - it is about the data being consistent between related tables, this is achived via Constraints, Validations, Triggers.
					-> Isolation - it is about concurrent requests being isolated and do not interfere with each other, achieved via different isolation levels set for transaction to control this behavior.
					-> Durability - this we have already seen, once a data is saved should be able to survive system failures, and persisted for long time for accessibility.
					
		-> How much latency we expect? (s, ms, microseconds)
		-> How many transactions we expect? (hundreds, thousands, millions of transactions per second).
		-> How much data we will store? (MB, GBs, TBs or PBs).
					
			
We will know some basics about Relational Databases
----------------------------------------------------
		-> This was the only option available a decade back.
		-> This provides with predefined set of Tables/Schemas to store data.
		-> We know about SQL which is a RDBM system -> which provides us strong relationship between entities/tables using foreign keys, gives us strong transactional capabilities to maintain consistency between different tables, Like all of the parts of transaction gets successful or everything rolled back if atleast one fails.
		-> PK helps as a unique key to identify each row / record from table. Each record can be called as each instance of an entity.	

It can be used for;
-------------------
	OLTP - Online Transaction Processing 
			-> Typically when we have a web application and transactions done on that, then that will be using OLTP.
	OLAP - Online Analytics Processing
			-> This is mostly used in Dataware house applications where a huge number of data can be stored and generate analytics/intelligence from it.
			
	Mainly OLAP or OLTP are design based category of database system.
	------------------------------------------------------------------
			-> How we design the schema, if we need short transactions to be run faster, efficiently, we do all CRUD operations or Read heavy system, if we use the data stored for Analytics or Intelligence purposes like viewing sales trends data, data analysis etc.,
			
			-> OLTP - designed for transactional workloads.
					- designed to have normalized schema to minimize redundancy, optimize transactional processing, ensure data integrity.
			-> OLAP - designed for analytics workloads
					- denormalized schema or star/snowflake schema designed to optimize data retrieval, support complex queries (multiple joins) and faciliate reporting and analysis purposes.
					
Exploring Relational Databases - OLTP:
=======================================
		Popular Relational Databases - MySQL, Oracle, SQL Server, PostgreSQL
			=> Azure SQL Database - Managed Microsoft SQL Server
			=> Azure Database for MySQL - Managed MySQL
			=> Azure Database for PostgreSQL - Managed PostgreSQL
			
			
Azure SQL Database:
--------------------
	-> Fully Managed Relational Database service offered by Azure. 
	-> Based on SQL server database engine.
	-> Provides Scalability and High Availability, Automatic Updates, Backups. - 99.99%.
	-> Hyperscale Storage (upto 100TB)
	
Azure Database for MySQL:
--------------------------
	-> Fully Managed, Scalable MySQL database.
	-> Provides Scalability and High Availability, Automatic Updates, Backups. - 99.99%.
		-> Can choose single zone or zone redundant high availability
	-> Typically used for LAMP Stack (Linux, Apache, MySQL, Perl/Python/PHP)
	
Azure Database for PostgreSQL:
-------------------------------
	-> Fully Managed, Scalable PostgreSQL database.
	-> Provides Scalability and High Availability, Automatic Updates, Backups. - 99.99%.
		-> Can choose single zone or zone redundant high availability
	-> Typically used for LAMP Stack (Linux, Apache, MySQL, Perl/Python/PHP)

Commands used in next sections:
-------------------------------
mysql --host=mysql-server-in28minutes.mysql.database.azure.com --user=mysqlserver -p
 
create database todos;
use todos;
create table user (id integer, username varchar(30) );
describe user;
insert into user values (1, 'Ranjith');
insert into user values (2, 'Ramesh');
select * from user;

Created a "Azure Database for MySQL" using Azure Portal ->	connected to it via cloud-shell commands provided above. 
username: Gadmin, Password: P@ssw0rd
hostname/server name: my-first-sql-server.mysql.database.azure.com
MySQL version: 8 

Created a database todos -> create user table -> inserted two rows (Ranjith, Ramesh)

Next, Lets start discuss about OLAP (Online Analytics Processing)
------------------------------------------------------------------
	From before sections, we understood, OLAP is mainly for data analytics based applications where huge data is stored(like petabytes) and complex queries are used for fetching the data for analysis and provide intelligence for the businesses.
		Examples: Reporting Applications, Dataware houses etc.,
		
			So these petabytes of data are mostly consolidated from multiple databases(including transactional databases).
	
	For this kind of application,"Azure Synapse Analytics" is the service offered by Azure.
	---------------------------------------------------------------------------------------
		-> It is a Petabyte-Scale distributed data ware house.
			-> Which includes capabilities of Data integration + Enterprise Datawarehousing + Big data Analytics
			-> Enables MPP (Massively parallel processing) - like running a query against a single server will be slow, but running it against multiple parallel nodes is faster.
			-> But how do we run the query against multiple nodes?
				This is where we have to understand how OLAP stores the data in a different way than OLTP and which helps for distributing the single query to run across multiple nodes to fetch data faster.
			
					OLTP: use row wise storage
					OLAP: use columnar storage.
					
				-> each columns are stored in different nodes which helps to store the data and helps retrieving it effectively because the column will have similar kind of data, helps in much better compression of data.
				-> most importantly it helps to run the very complex queries against multiple nodes to fetch the data very efficiently.
				

NoSQL Database:
---------------
	-> it means "not only SQL".
	-> main feature of NoSQL database is it does enforce fixed schema to be required for data storing, even without schema, flexible schema which can evlove over a period of time for data storing.
	-> Meaning each row of the table/collection can have different types, number of columns, relationship with other tables, everything can be different, which is not in the case with SQL for maintaining the data integrity.

What does it help in achieving?
--------------------------------
	-> It helps if we think / not able to decide what should be schema of our tables, that should not stop us from developing applications and deploying, we can go ahead with what information we have right now, and over a period of time based on the application needs, the schema can be changed without much problem.
	
			"Also to note this does come with the trade off "Consistency and SQL features" to provides more Scalability and High Performance"
	
	->Horizontally scale to petabytes of data with millions of Transaction per second (TPS).
	
Which is the Azure Managed Service?
------------------------------------
	Azure Cosmos DB - Fully Managed NoSQL database service.
		Automatically replicates data across multiple data across multiple Azure regions.
		-> Schemaless
		-> 99.999% Availability
		-> Single digit millisecond response times
		-> Automatic Scaling (Serverless).

Supports APIs for MongoDB (document), Cassandra(key/value), Gremlin (Graph).

Create a Azure Cosmos DB for MongoDB APIs
------------------------------------------
		It gives the details how do we connect Cosmos DB to different applications built on .Net, NodeJs, Java, MongoDB Shell with instructions.
		
		Also we have a way of connecting to Cosmos DB via Data Explorer way. 
			-> We can create a new collection -> new document saving data and when we do that we also mention shard key (using which data gets partitioned to multiple nodes) to enable distributed database system.
			
Azure In Memory Database:
--------------------------
	-> Retrieving data from in memory is always faster than retrieving data from disk.
	-> But all our databases so far we have seen stores data in disk, because we want the data to be persistent, don't want any data to be lost because of any machine restart or any such actions.
	-> But if we want data to be retrieved very faster as well as we want data to be persistent, then we have storages like "Redis" to do that for us.
	
Azure Managed Service for Redis - Azure Cache for Redis
========================================================
		-> Delivers response in microsecond latency.
		-> It provides upto 99.999% availability by replicating data across multiple zones and regions.
							
			
NETWORKING IN AZURE
====================
	In the above sections, we have learnt about Compute, Storage, Database services in Azure, But how do we ensure that all the resources we create are secure enough that it cannot be accessed outside without sufficient permissions.
	
	That is where we would need to understand the concepts of networking in Azure.
	We would learn about "Azure Virtual Network", why do we need multiple subnets, how do we create firewalls?.
	
Azure Virtual Network:
-----------------------
	-> What is a virtual network?
		-> We know that all the communication between the resources / machines occur with help of network.
	
In a Corporate Network or an On-Premise data center:
----------------------------------------------------	
Consider we have an Application talking to database within a corporate data center;
	Here comes a question;
		-> Can anyone on the internet see the data exchange between the application and the database?
			The answer is NO.
		-> Can some one directly connect to the database from internet?
			NO again.
			
	If we want to connect to the database, we would need to connect to the Corporate Network first and then would be able to access the Application and the database.

		-> So Corporate Network is the one that provides us the secure internal network to protect the resources, data and communication from external users.
		
How do we create our own private network in the Cloud?
------------------------------------------------------
	Via Azure Virtual Network
		-> It is your own isolated network within Azure.
		-> Network traffic within the virtual network is not visible to other Azure Virtual Networks meaning it is also not visible outside of Azure.
		-> Each Virtual Network that we create would be associated with One Region, when we create a virtual network, we create it inside a region.
		-> we can also have a multiple virtual networks created for different regions.
		
	=> Basically, when we create our resources (Compute, Storage, Databases etc.,) we should have it created within the Virtual Network (Best Practice).	
	=> So we can control all traffic coming in and going outside our Virtual network, trying to communicate to our resources.
		-> It basically helps us with restricting Unauthorized accesses to our resources.
		-> Enables the secure connection between our cloud resources.
		-> Ensures the communication between cloud resources within a virtual network is not visible outside the virtual network.
		
Understanding Need for Subnets
===============================
	Inside Virtual Network we can create multiple subnets, but what is subnets?		
	Subnet - short form for Subnetwork, which is a smaller network created by dividing a larger network into segments.
		-> So we can have different resources created within a Virtual Network like Compute, Storage, Load balancers etc.,
			-> For ex; We want Load balancer to be accessible from Internet to connect to our virtual network then provide access to the applications or databases.
			-> At the same time, our applications/databases should not be accessible directly from Internet, should restrict Unauthorized accesses.
				-> basically VMs or DBs should be private resources.
				-> Load balancer to be public resource.
				
	How do we seperate public resources from private resources inside a Virtual network?
	-------------------------------------------------------------------------------------
		We can have different subnets created with in a virtual network to seperate resources and we can have the public resources in a subnet and private resources in another subnet.
		
		These subnets within a VNet can communicate each other via Route tables, but outside VNet only public resource can be accessible.

				-> By this we can have the neccessary configurations setup to make our private resources more protected.
				
Creating Virtual Network in Portal
===================================
	Go to Portal -> Virtual Network -> Create Virtual Network -> And in the Ip Address section, we can see the subnets, we can add multiple new subnets as well (name it like private, public) with address range.
		For eg;
		--------
			private: 10.0.0.0 - 10.0.0.255 (251 + 5 Azure reserved addresses)
			public: 10.0.1.0 - 10.0.1.255 (251 + 5 Azure reserved addresses)
			
	There are several other options like services but this is not sure, I think may be when we select multiple services, those service end points will have access to the resources in our Vnet (subnet)

We can also create resources within the specific subnet, for eg; Create Virtual Machine ->	Choose the resource group where Vnet is created (my-first-virtual-network) -> Go to Networking -> Here we will have the option to choose the subnets where we want to this have this resources deployed / part of.

We will have more details around this in next step.

Whenever we create a VM in a Vnet, by default it is assigned to the private subnet, we can also change it to public subnet, it will assign the private IP address within the subnet's range thats selected.

	VMs within the same Vnet can communicate using private IP addresses.
	We can also configure Public IP addresses to the resources, point to note is we need to pay extra for using public IP address, there are no additional charges for using private IP address.
	
	Different VMs in the Vnet can communicate each other via private IP addresses, though they are in different subnets (private ore public).
	
Suppose we have another Vnet and we want resources in two Vnets to be communicate each other. How is it possible??????

	-> Network Peering Relationship. go to Vnet -> Peerings -> Setup the configuration ton establish the relationship between two Vnets.
	
	With help of this, different resources in different Vnets can talk to each other, interesting thing to note is these Peering Vnets can be in different regions as well.
	
	Peering link we need to create and check again ???
	
 	
How to make a Vnet or Subnet a publicly accessible one?
========================================================
	-> We can do so by configuring inbound/outbound network traffic rules through which we can set the protocol by which the resource can be accessible outside the Azure. (SSH, HTTPS, RDP,HTTP).
	-> We can assign public address to the resource (VM, Load balancer, etc.,)

What is a Static IP Addresses?
-------------------------------
	-> Static IP Address is one which is assigned to a device/resource manually and it remains the same for the resource to be accessed with in the network.
	-> The same in case of Dynamic IP Addresses, everytime the resource connects to network gets assigned a new IP address which is done by DHCP (Dynamic Host Control Protocol).
		
			-> Static IP Address are useful when we want the IP address of a resource to not to be changing, and we want to connect to the resource with the same IP address without relying on DHCP renewals.
			-> To ensure better continuous accessibility, this would be more useful.
	
	In a Subnet created, there can be some public IP addresses as well as some private IP addresses.
	
Let us see more on the Azure Network Security Features:
=======================================================
Azure DDoS Protection
----------------------
		What is DDoS - Distributed Denial of Service 
		---------------------------------------------
			-> It is an attack that tries to send huge amount of traffic to the target service or server to cause unavailablity/performance degradation to the application/service. 
			-> Can also cause huge bill amount if auto-scaling is enabled.
			
	To prevent this Azure offers two DDoS protection features, 
	Basic
	-----
		-> Protects against common network layer attacks
		-> By default enabled
		-> No additional cost involved
		-> It automatically identifies the type of DDoS attack and blocks it.	
		
	Standard
	--------	
		-> We need to enable it by - go to virtual network resource -> DDoS Protection -> Enable it.
			-> it is a paid service to protect public IP addresses within Vnet.
			-> It includes alerting when attacks occur, metrics, analytics, reporting.
			-> Support from DDoS Protection Rapid Response (DRR) team.
			-> Cost guarantee by receiving service credits if DDoS attack is not able to be prevented or due to that bill amount increased.
			
AZURE FIREWALL
--------------
	-> Azure Firewall is a "Managed Network Security Service" to provide Network Level Protection to the resources.
		-> It is centrally configured and managed, so we can associate one Firewall to multiple Vnets (across subscription) to provide security to all the resources associated with it.
		-> Basically it provides protection/filtering based on the inbound rules that can be configured within firewall so that only the trusted request pass on the firewall to access and get the response.
		-> It can be integrated with several Azure Services like Azure Monitor to view the logs, analytics of the inbound requests blocked/allowed or more.
		-> It is a service with built in high availability and unrestricted cloud scalability.
		
	There is also another known Web Application Firewall (WAF) - which is basically for protection specific web application by preventing common web application attacks like XSS, SQL Injection and OWASP listed top 10 web application attacks. 	
			-> It is tied to http/https protocol.
	
	
AZURE NETWORK SECURITY GROUPS
==============================
	-> 	We have learnt about Azure Firewall which secures from outside of our Vnet with the configured inbound / outbound rules to connect to Vnets. (External Firewall)
	-> But Network Security Groups all about securing within Virtual Network, that is when we create a VM, we can set inbound rules like (SSH, HTTP, HTTPs) to connect to that machine, by deployment it automatically creates a Network Security Group resource at the background, (vn-vm-nsg) There we can view the inbound rules, outbound rules configured.
		-> This is INTERNAL FIREWALL right before the resources.
		-> We can configure within a vnet if suppose we don't a certain VM to connect to a Specific Subnet/VM it can be configured at NSG level.	
			
		-> Allow or block traffic based on source/destination IP address, Protocol, Port.
		-> NSGs are typically attached with subnets, network interfaces.
		
	Use Cases:
	-----------
		-> We would want to restrict DB server access to only certain IP address ranges.
		-> If we have a VM which has all downloadable packages, documents, but we want it to downloadable to only certain IP address ranges, not for all, as we don't all machines to be able to download the files. (Outbound request security rule)
		
		By default DenyAllInBound, lower the number higher the priority.
		
Inbound Security Rules
-----------------------		
		300 - SSH
		320 - HTTP
		65000 - AllowVnetInBound
		65001 - AllowAzureLoadBalancerInBound
		65500 - DenyAllInBound

Outbound Security Rules	
------------------------
		65000 - AllowVnetOutBound
		65001 - AllowInternetOutBound
		65500 - DenyAllOutBound	
		
		